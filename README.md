# Graph Cross — среда и эксперименты с RL

В данном репозитории представлена синтетическая логическая среда **Graph Cross** и эксперименты по обучению LLM-агента с использованием reinforcement learning с верифицируемым бинарным вознаграждением.

Работа выполнена в формате исследовательского задания с фокусом на:
- дизайн среды,
- постановку RL-эксперимента,
- анализ динамики обучения и причин отсутствия прогресса при разреженном reward.

---

## Структура репозитория

```

.
├── games/
│   └── graph_cross/
│       ├── env.py            # Реализация среды (Env)
│       ├── verifier.py       # Логика проверки ответа (Verifier)
│       ├── prompt.py         # Шаблон промпта задачи (на английском)
│       └── generator.py      # Генерация задач и датасетов
│
├── data/
│   ├── train/                # Обучающие датасеты
│   └── test/                 # Фиксированные тестовые датасеты
│
├── REPORT.md                 # Подробный отчет о решении и экспериментах
├── README.md                 # Краткое описание проекта
└── .gitignore

```

---

## Отчет

Подробное описание задачи, среды, конфигураций обучения, логов и анализа результатов приведено в файле **REPORT.md**.

---

## Обучение модели

Код обучения и полные логи экспериментов представлены в Google Colab ноутбуке:

**[Colab link here]**

Ноутбук содержит:
- генерацию датасетов,
- запуск GRPO-тренера,
- логи обучения,
- промежуточные эксперименты с упрощением конфигурации.

---

## Примечание

В рамках экспериментов модель не продемонстрировала улучшения качества даже на упрощённых конфигурациях задач.  
Это рассматривается как осмысленный отрицательный результат и подробно анализируется в отчёте.

---

## Используемые технологии

- Python
- PyTorch
- HuggingFace Transformers
- Unsloth
- GRPO (Reinforcement Learning with Verifiable Rewards)
